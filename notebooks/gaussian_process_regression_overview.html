<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="John Waczak">
<meta name="dcterms.date" content="2023-02-03">

<title>Gaussian Processes and Gaussian Process Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="gaussian_process_regression_overview_files/libs/clipboard/clipboard.min.js"></script>
<script src="gaussian_process_regression_overview_files/libs/quarto-html/quarto.js"></script>
<script src="gaussian_process_regression_overview_files/libs/quarto-html/popper.min.js"></script>
<script src="gaussian_process_regression_overview_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="gaussian_process_regression_overview_files/libs/quarto-html/anchor.min.js"></script>
<link href="gaussian_process_regression_overview_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="gaussian_process_regression_overview_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="gaussian_process_regression_overview_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="gaussian_process_regression_overview_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="gaussian_process_regression_overview_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Gaussian Processes and Gaussian Process Regression</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>John Waczak </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 3, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="introduction" class="level1 page-columns page-full">
<h1>Introduction</h1>
<div class="page-columns page-full"><p>The following is based on the book <strong>Gaussian Processes for Machine Learning</strong> by <em>Carl Edward Rasmussen and Christopher K. I. Williams</em>. You can find the free online book <a href="https://gaussianprocess.org/gpml/">here</a>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Carl Edward Rasmussen and Christopher K. I. Williams; The MIT Press, 2006. ISBN 0-262-18253-X.</p></li></div></div>
<p>To explain/derive the Gaussian Process model for regression, let’s first consider a motivated example: <strong>Linear Regression</strong>. We will use this guiding example to derive GRP from a <em>weight space view</em>. After this derivation, will suggest a simpler, but more abstract derivation using a <em>function space view</em>.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Pkg</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">Pkg</span>.<span class="fu">activate</span>(<span class="st">"."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>  Activating project at `~/gitrepos/machine-learning/MLJGP.jl/notebooks`</code></pre>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">LinearAlgebra</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Distributions</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Random</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Plots</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>First let’s set up some data we can use for training:</p>
</section>
<section id="weight-space-view" class="level1 page-columns page-full">
<h1>Weight-Space View</h1>
<section id="nomenclature" class="level2">
<h2 class="anchored" data-anchor-id="nomenclature">Nomenclature</h2>
<p>We consider a dataset <span class="math inline">\mathcal{D}</span> with <span class="math inline">n</span> observations <span class="math display">\begin{equation}
    \mathcal{D} = \Big\{ (\mathbf{x}_i, y_i) \;\Big\vert \; i = 1,...,n\Big\}
\end{equation}</span></p>
<ul>
<li><span class="math inline">\mathbf{x}_i</span> is the <span class="math inline">i^{th}</span> D-dimensional input (feature) vector</li>
<li><span class="math inline">y_i</span> is the <span class="math inline">i^{th}</span> target</li>
</ul>
<p>Linear regression is easily understood in terms of <em>linear algebra</em>. We therefore collect our dataset <span class="math inline">\mathcal{D}</span> into a <span class="math inline">D \times n</span> dimensional <a href="https://en.wikipedia.org/wiki/Design_matrix"><strong>Design Matrix</strong></a>. Note that we have used a transposed definition (features are rows, records are columns) as Julia is a column-major language (like Matlab &amp; Fortran).</p>
<p><span class="math display">\begin{equation}
    X := \begin{pmatrix}
    \vdots &amp; \vdots &amp; &amp; \vdots \\
    \mathbf{x}_1 &amp; \mathbf{x}_2 &amp; ... &amp; \mathbf{x}_n \\
    \vdots &amp; \vdots &amp; &amp; \vdots
    \end{pmatrix}
\end{equation}</span></p>
<p>and our targets into a target vector</p>
<p><span class="math display">\begin{equation}
    \mathbf{y} := (y_1, ..., y_n)
    \end{equation}</span> so that the full training set becomes <span class="math display">\begin{equation}
    \mathcal{D} := (X, \mathbf{y})
\end{equation}</span></p>
</section>
<section id="standard-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="standard-linear-regression">Standard Linear Regression</h2>
<p>Standard linear regression is a model of the form <span class="math display">\begin{equation}
    f(\mathbf{x}) = \mathbf{x}^T\mathbf{w}
\end{equation}</span> where <span class="math inline">\mathbf{w}</span> is the <span class="math inline">D</span>-dimensional vector of weights. By minimizing the mean-squared-error between our model and targets, one can show that the optimal weights are given by <span class="math display">\begin{equation}
    \mathbf{w} = (XX^T)^{-1}X\mathbf{y}
\end{equation}</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This can also be easily obtained geometrically by finding the vector with the shortest distance to the hyperplane defined by the column space of <span class="math inline">X</span>. This corresponds to solving the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Normal_equations"><strong>normal equations</strong></a> <span class="math display">\begin{equation}
        XX^T \mathbf{w} = X\mathbf{y}
    \end{equation}</span></p>
</div>
</div>
</div>
<p>The following demonstrates this procedure on a simple dataset</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="fu">rand</span>(<span class="fl">200</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fl">0.3</span> <span class="op">.*</span> X <span class="op">.+</span> <span class="fl">0.25</span> <span class="op">.*</span> (<span class="fu">rand</span>(<span class="fl">200</span>) <span class="op">.-</span> <span class="fl">0.5</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X<span class="op">'</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> (X<span class="op">*</span>X<span class="op">'</span>)<span class="fu">\</span>(X<span class="op">*</span>y)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>ypred <span class="op">=</span> X<span class="op">'*</span>w</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X<span class="op">'</span>, y, </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    seriestype<span class="op">=:</span>scatter,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=:</span>red, alpha<span class="op">=</span><span class="fl">0.25</span>, </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"data"</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(X<span class="op">'</span>, ypred, color<span class="op">=:</span>blue, label<span class="op">=</span><span class="st">"fit"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="fu">xlabel!</span>(<span class="st">"x"</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="fu">ylabel!</span>(<span class="st">"y"</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="fu">title!</span>(<span class="st">"Linear Regression in 1-Dimension"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<p><img src="gaussian_process_regression_overview_files/figure-html/cell-4-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We can also fit a y-intercept (aka <em>bias</em>) by augmenting the design matrix <span class="math inline">X</span> to contain an extra row with all <code>1</code>’s, i.e.&nbsp; <span class="math display">\begin{equation}
    X[D+1, :] = (1, ..., 1)
\end{equation}</span></p>
</div>
</div>
</div>
</section>
<section id="making-it-bayesian" class="level2">
<h2 class="anchored" data-anchor-id="making-it-bayesian">Making it Bayesian</h2>
<p>Standard lienar regression assumes that are data <span class="math inline">\mathcal{D}</span> are perfect but we can clearly see that the above data are noisy. To account for this, we need to make our model <em>Bayesian</em> by augmenting it to consider measurement error. We define <span class="math display">\begin{align}
    f(\mathbf{x}) &amp;= \mathbf{x}^T\mathbf{w} \\
    \mathbf{y} &amp;= f(\mathbf{x}) + \mathbf{\epsilon} \\
    \mathbf{\epsilon} &amp;\sim \mathcal{N}(0, \sigma_n^2)
\end{align}</span> or, in words, our observed values differ from the <em>truth</em> by identically, independently, distributed Gaussian noise with mean <span class="math inline">0</span> and variance <span class="math inline">\sigma_n^2</span>. The assumption that the noise is i.i.d. is critical because it allows us to simplify the <em>likelihood</em> function by separating out each individual contribution by our datapoints: <span class="math display">\begin{align}
    p(\mathbf{y}\vert X,\mathbf{w}) &amp;:= \prod\limits_i^n p(\mathbf{y}_i \vert \mathbf{x}_i, \mathbf{w}) \\
    &amp;= \prod\limits_i^n \frac{1}{\sqrt{2\pi\sigma_n^2}}\exp\left( -\dfrac{(\mathbf{y}_i-\mathbf{x}_i^T\mathbf{w})^2}{2\sigma_n^2}\right)\\
    &amp;= \dfrac{1}{(2\pi\sigma_n^2)^{n/2}}\exp\left( -\frac{1}{2\sigma_n^2}\lvert \mathbf{y} - X^T\mathbf{w}\rvert^2 \right) \\
    &amp;= \mathcal{N}\left(X^T\mathbf{w}, \sigma_n^2I\right)
\end{align}</span></p>
<p>To perform inference with this updated model, we apply Baye’s Rule, that is:</p>
<p><span class="math display">\begin{equation}
    p(\mathbf{w}\vert \mathbf{y}, X) = \dfrac{p(\mathbf{y}\vert X, \mathbf{w})p(\mathbf{w})}{p(\mathbf{y}\vert X)}
\end{equation}</span> where</p>
<ul>
<li><span class="math inline">p(\mathbf{w}\vert \mathbf{y}, X)</span> is the <strong>posterior distribution</strong></li>
<li><span class="math inline">p(\mathbf{y}\vert X, \mathbf{w})</span> is the <strong>likelihood</strong></li>
<li><span class="math inline">p(\mathbf{w})</span> is the <strong>prior distribution</strong></li>
<li><span class="math inline">p(\mathbf{y} \vert X)</span> is the <strong>marginal likelihood</strong>, i.e.&nbsp;the normalization constant</li>
</ul>
<p>It is now that the utility of choosing gaussian distributions for our likelihood and prior becomes clear… <span class="math display">\begin{align}
    p(\mathbf{w}\vert\mathbf{y},X) &amp;\propto \exp\left(-\frac{1}{2\sigma_n^2}(\mathbf{y}-X^T\mathbf{w})^T(\mathbf{y}-X^T\mathbf{w}) \right)\exp\left(-\frac{1}{2}\mathbf{w}^T\Sigma_p^{-1}\mathbf{w}\right)
    \end{align}</span> Taking the log and expanding leads to <span class="math display">\begin{align}
    \log(p(\mathbf{w}\vert \mathbf{y}, X))&amp;= \frac{1}{2}\left[ \frac{1}{\sigma_n^2}\mathbf{y}^T\mathbf{y} - \frac{1}{\sigma_n^2}\mathbf{y}^TX^T\mathbf{w} - \frac{1}{\sigma_n^2}\mathbf{w}^TX\mathbf{y} + \frac{1}{\sigma_n^2}\mathbf{w}^TXX^T\mathbf{w} + \mathbf{w}^T\Sigma_p^{-1}\mathbf{w}\right] \\
    &amp;= \frac{1}{2}\left[ \mathbf{w}^T\left(\frac{1}{\sigma_n^2}XX^T+\Sigma_p^{-1}\right)\mathbf{w} -\left(\frac{1}{\sigma_n^2}\mathbf{y}^TX^T\right)\mathbf{w} - \mathbf{w}^T\left(\frac{1}{\sigma_n^2}X\mathbf{y}\right)  + \mathbf{y}^T\frac{1}{\sigma_n^2}\mathbf{y}\right]\\
    &amp;= \mathbf{w}^TA\mathbf{w} - B^T\mathbf{w} - \mathbf{w}^TB + C
\end{align}</span> where we have defined <span class="math display">\begin{align}
    A &amp;:= \frac{1}{\sigma_n^2}XX^T + \Sigma_p^{-1} \\
    B &amp;:= \frac{1}{\sigma_n^2}X\mathbf{y} \\
    C &amp;:= \mathbf{y}^T\frac{1}{\sigma_n^2}\mathbf{y}
\end{align}</span></p>
<p>Now we can complete the square so that <span class="math display">\begin{equation}
    \mathbf{w}^TA\mathbf{w} - B^T\mathbf{w} - \mathbf{w}^TB + C = \left(\mathbf{w} - \bar{\mathbf{w}} \right)^TA\left(\mathbf{w} - \bar{\mathbf{w}} \right) + K
    \end{equation}</span> leading to <span class="math display">\begin{align}
    \bar{\mathbf{w}} &amp;= A^{-1}B = \frac{1}{\sigma_n^2}\left(\frac{1}{\sigma_n^2}XX^T + \Sigma_p^{-1}\right)^{-1}X\mathbf{y} \\
    K &amp;= C- \bar{\mathbf{w}}^TA\bar{\mathbf{w}}
\end{align}</span> Since <span class="math inline">K</span> does not depend on <span class="math inline">\mathbf{w}</span> directly, it may be absorbed into the normalization of <span class="math inline">p(\mathbf{w}\vert \mathbf{y}, X)</span>. Thus we are left with</p>
<p><span class="math display">\begin{align}
    p(\mathbf{w}\vert\mathbf{y},X) &amp;= \mathcal{N}\left( \bar{\mathbf{w}}=\frac{1}{\sigma_n^2}A^{-1}X\mathbf{y}, \Sigma=A^{-1}\right) \\
    A &amp;= \frac{1}{\sigma_n^2}XX^T+\Sigma_p^{-1}
\end{align}</span></p>
<p>This result gives us the gaussian distriubtion over the space of possible parameter vectors <span class="math inline">\mathbf{w}</span>. To use this distribution to make predictions, consider a newly supplied testpoint <span class="math inline">\mathbf{x}_*</span>. We want to find <span class="math display">\begin{equation}
    p(y_* \vert \mathbf{x}_*, \mathbf{y}, X)
\end{equation}</span></p>
<p>We do this by marginalizing over our weight distribution, i.e.&nbsp; <span class="math display">\begin{equation}
    p(y_* \vert \mathbf{x}_*, \mathbf{y}, X) = \int_{\mathbf{w}} p(y_*\vert \mathbf{x}_*,\mathbf{w})p(\mathbf{w}\vert \mathbf{y}, X)d\mathbf{w}
\end{equation}</span> If we make the further assumption that testing points are i.i.d. guassian distriubted, we see that this integral is the product of two gaussians and therefore is also a guassian. To find the mean and covariance of the predictive distribution, we check <span class="math display">\begin{align}
    \bar{y}_* &amp;= \mathbb{E}[y_*] = \mathbb{E}[\mathbf{x}_*^T\mathbf{w}] = \mathbf{x}_*^T\mathbb{E}[\mathbf{w}] = \mathbf{x}_*^T\bar{\mathbf{w}} \\
    \text{Cov}(y_*) &amp;= \mathbb{E}[(y_*-\bar{y}_*)(y_*-\bar{y}_*)^T] \\
    &amp;= \mathbb{E}[(\mathbf{x}_*^T\mathbf{w}-\mathbf{x}_*^T\bar{\mathbf{w}})(\mathbf{x}_*^T\mathbf{w}-\mathbf{x}_*^T\bar{\mathbf{w}})^T] \\
    &amp;= \mathbb{E}[\mathbf{x}_*^T(\mathbf{w}-\bar{\mathbf{w}})(\mathbf{w}-\bar{\mathbf{w}})^T\mathbf{x}_*] \\
    &amp;= \mathbf{x}_*^T\mathbb{E}[(\mathbf{w}-\bar{\mathbf{w}})(\mathbf{w}-\bar{\mathbf{w}})^T]\mathbf{x}_* \\
    &amp;= \mathbf{x}_*^T\text{Cov}(\mathbf{w})\mathbf{x}_* \\
    &amp;= \mathbf{x}_*^TA^{-1}\mathbf{x}_*
\end{align}</span> so that <span class="math display">\begin{equation}
    \boxed{p(y_* \vert \mathbf{x}_*, \mathbf{y}, X) = \mathcal{N}\left(\mathbf{x}_*^T\mathbf{w},\;  \mathbf{x}_*^TA^{-1}\mathbf{x}_*\right)}
\end{equation}</span></p>
</section>
<section id="doing-more-with-less-kernelization" class="level2">
<h2 class="anchored" data-anchor-id="doing-more-with-less-kernelization">Doing More with Less: Kernelization</h2>
<p>Let’s take a break from our Bayesian regression and return to the standard linear regression model for a moment. The key drawback of linear models like this is, of course, that they’re <em>linear</em>!. Considering that many (most?) <em>interesting</em> relationships are not linear, how can we extend our simple linear model to enable us to perform complicated non-linear fits?</p>
<p>In the parlance of machine learning, the simple solution is to do <a href="https://en.wikipedia.org/wiki/Feature_engineering"><strong>feature engineering</strong></a>. If our inital feature vector is <span class="math display">\begin{equation}
    \mathbf{x} = (x_1, ..., x_n)
\end{equation}</span> we can use our <em>expertise</em> to concot new combinations of these features to produce the agumented vector <span class="math display">\begin{equation}
    \tilde{\mathbf{x}} = (x_1, ..., x_n, x_1^2, \;sin(x_2), \;x_5x_7/x_4,\;...)
\end{equation}</span></p>
<p>As an example, a linear classifier is unable to distinguish points inside a circle from those outside just from the <span class="math inline">(x,y)</span> coordinates alone. Augmenting the feature vector to include the squared radius <span class="math inline">x^2+y^2</span> as a new feature removes this obstacle.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This works because the <em>linear</em> part of linear regression only refers to the fact that our model takes <em>linear combinations</em> of feature variables to produce it’s output. There is no restriction that the features themselves need to be independent variables! This same idea is what makes methods like <a href="https://www.pnas.org/doi/10.1073/pnas.1517384113">SINDy</a> work…</p>
</div>
</div>
</div>
<p>Constructing new features is often more art than science. To standardize the process, let’s abstract mapping from the original feature vector <span class="math inline">\mathbf{x}</span> to the augmented vector <span class="math inline">\tilde{\mathbf{x}}</span>. This is accomplished via the projection map <span class="math inline">\phi:\mathbb{R}^D \to \mathbb{R}^N</span> where <span class="math display">\begin{equation}
    \mathbf{x} \mapsto \tilde{\mathbf{x}} = \phi(\mathbf{x})
\end{equation}</span></p>
<p>The result is that our linear model updates to become <span class="math display">\begin{equation}
    f(\mathbf{x}) := \phi(\mathbf{x})^T\mathbf{w}
\end{equation}</span> where the weight vector has gone from <span class="math inline">D</span> dimensional to <span class="math inline">N</span> dimensional.</p>
<p>Similarly, the normal equations for <span class="math inline">\mathbf{w}</span> update to become <span class="math display">\begin{equation}
    \mathbf{w} = (\Phi\Phi^T)^{-1}\Phi\mathbf{y}
\end{equation}</span> where <span class="math inline">\Phi = \phi(X)</span> is the <span class="math inline">N\times n</span> matrix resulting from applying <span class="math inline">\phi</span> columnwise to <span class="math inline">X</span>.</p>
<p>The following example shows how to use such a mapping to produce a quadratic polynomial fit.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generate noisy quadratic data</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="fl">2</span><span class="fu">*rand</span>(<span class="fl">200</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span>  <span class="fl">1.0</span> <span class="op">.-</span> <span class="fl">2.0</span> <span class="op">.*</span> X <span class="op">.+</span> X<span class="op">.^</span><span class="fl">2</span> <span class="op">.+</span> (<span class="fl">0.5</span> <span class="op">.*</span> <span class="fu">rand</span>(<span class="fl">200</span>))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X<span class="op">'</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># compute projection</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="fu">Φ</span>(x) <span class="op">=</span> [<span class="fl">1.0</span>, x<span class="op">...</span>,[x[i]<span class="op">*</span>x[j] for i<span class="op">∈</span><span class="fl">1</span><span class="op">:</span><span class="fu">size</span>(x,<span class="fl">1</span>) for j<span class="op">∈</span><span class="fl">1</span><span class="op">:</span><span class="fu">size</span>(x,<span class="fl">1</span>)]<span class="op">...</span>]</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>X̃ <span class="op">=</span> <span class="fu">hcat</span>([<span class="fu">Φ</span>(col) for col <span class="op">∈</span> <span class="fu">eachcol</span>(X)]<span class="op">...</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the parameters</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> (X̃<span class="op">*</span>X̃<span class="ch">')\(X̃*y)</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># compute projection on test points</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>Xpred <span class="op">=</span> <span class="fu">collect</span>(<span class="fl">0</span><span class="op">:</span><span class="fl">0.025</span><span class="op">:</span><span class="fl">2</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>Xpred <span class="op">=</span> Xpred<span class="op">'</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>X̃pred <span class="op">=</span> <span class="fu">hcat</span>([<span class="fu">Φ</span>(col) for col <span class="op">∈</span> <span class="fu">eachcol</span>(Xpred)]<span class="op">...</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co"># compute prediction on test points</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>ypred <span class="op">=</span> X̃pred<span class="op">'*</span>w</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X<span class="op">'</span>, y, </span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    seriestype<span class="op">=:</span>scatter,</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=:</span>red, alpha<span class="op">=</span><span class="fl">0.25</span>, </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"data"</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(Xpred<span class="op">'</span>, ypred, color<span class="op">=:</span>blue, label<span class="op">=</span><span class="st">"fit"</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="fu">xlabel!</span>(<span class="st">"x"</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="fu">ylabel!</span>(<span class="st">"y"</span>)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="fu">title!</span>(<span class="st">"equation fit: </span><span class="sc">$</span>(<span class="fu">round</span>(w[<span class="fl">1</span>],digits<span class="op">=</span><span class="fl">2</span>))<span class="st"> + </span><span class="sc">$</span>(<span class="fu">round</span>(w[<span class="fl">2</span>], digits<span class="op">=</span><span class="fl">2</span>))<span class="st">x + </span><span class="sc">$</span>(<span class="fu">round</span>(w[<span class="fl">3</span>], digits<span class="op">=</span><span class="fl">2</span>))<span class="st">x²"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<p><img src="gaussian_process_regression_overview_files/figure-html/cell-5-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>We see from the above that our linear regression model found a great fit for a 2nd order polynomial when supplied with polynomial features.</p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>There is a <em>massive</em> problem with this method. Our order 2 polynomial map <span class="math inline">\phi</span> takes us from a <span class="math inline">D</span> dimenional feature vector to <span class="math inline">(D+1)!</span> many. This means that as we add more features to our feature map, the dimension of the resulting vector will quickly become prohibitively large.</p>
</div>
</div>
</div>
</section>
<section id="bayesian-regression-with-feature-mappings" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-regression-with-feature-mappings">Bayesian Regression with Feature Mappings</h2>
<p>Let’s update our Bayesian regression scheme to reflect the use of our feature projection map <span class="math inline">\phi</span>. First we define <span class="math display">\begin{align}
    \Phi &amp;:= \phi(X) \\
    \phi_* &amp;:= \phi(\mathbf{x}_*)
\end{align}</span></p>
<p>Our predictive distribution therefore becomes <span class="math display">\begin{align}
    p(y_* \vert \mathbf{x}_*, X, \mathbf{y}) &amp;= \mathcal{N}\left(\frac{1}{\sigma_n^2}\phi_*^TA^{-1}\Phi\mathbf{y}, \;\phi_*^TA^{-1}\phi_*\right) \\
    A &amp;= \frac{1}{\sigma_n^2}\Phi\Phi^T + \Sigma_p^{-1}
\end{align}</span> Great! Now we can do our Bayesian inference with non-linear features given by <span class="math inline">\phi</span>.</p>
<p>Returning to the problem of the rapidly-growing dimensionality of our augmented feature vectors <span class="math inline">\phi(\mathbf{x})</span>, we see that the computational bottleneck is the matrix inversion of <span class="math inline">A</span> which requires we invert an <span class="math inline">N\times N</span> matrix. Our prediction (i.e.&nbsp;the mean) involves multiplication on the right by the <span class="math inline">n</span> dimensional vector <span class="math inline">\mathbf{y}</span>. With that in mind, perhaps we can reformulate the above into an equivalent form using at most an <span class="math inline">n\times n</span> dimensional matrix…</p>
<p>Let <span class="math inline">K:= \Phi^T\Sigma_p\Phi</span>. Observe the following: <span class="math display">\begin{align}
    \frac{1}{\sigma_n^2}\Phi(K+\sigma_n^2I) &amp;= \frac{1}{\sigma_n^2}\Phi\left(\Phi^T\Sigma_p\Phi + \sigma_n^2I \right) \\
    &amp;= \frac{1}{\sigma_n^2}\Phi\Phi^T\Sigma_p\Phi + \Phi I \\
    &amp;= \left(\frac{1}{\sigma_n^2}\Phi\Phi^T \right)\Sigma_p\Phi + \left(\Phi I \Phi^{-1}\Sigma_p^{-1} \right)\Sigma_p\Phi \\
    &amp;= \left(\frac{1}{\sigma_n^2}\Phi\Phi^T + \Sigma_p^{-1}\right)\Sigma_p\Phi \\
    &amp;= A\Sigma_p\Phi
\end{align}</span></p>
<p>From there we see that <span class="math display">\begin{align}
    A^{-1}\frac{1}{\sigma_n^2}\Phi\left(K+\sigma_n^2I\right) &amp;= \Sigma_p\Phi \\
    \Rightarrow \frac{1}{\sigma_n^2}A^{-1}\Phi &amp;= \Sigma_p\Phi\left(K + \sigma_n^2I\right)^{-1} \\
    \Rightarrow \frac{1}{\sigma_n^2}\phi_*^TA^{-1}\Phi &amp;= \phi_*^T\Sigma_p\Phi\left(K + \sigma_n^2I\right)^{-1}
\end{align}</span></p>
<p>For the covariance, we utilize the matrix inversion lemma which states <span class="math display">\begin{equation}
    (Z + UWV^T)^{-1} = Z^{-1} - Z^{-1}U(W^{-1} + V^TZ^{-1}U)^{-1}V^TZ^{-1}
\end{equation}</span></p>
<p>With the identification <span class="math display">\begin{align}
    Z^{-1} &amp;\to \Sigma_p \\
    W^{-1} &amp;\to \sigma_n^2I \\
    V &amp;\to \Phi \\
    U &amp;\to \Phi
\end{align}</span> we find <span class="math display">\begin{align}
    \Sigma_p - \Sigma_p\Phi\left(\Sigma_p + \Phi^T\Sigma_p\Phi \right)^{-1}\Phi^T\Sigma_p  &amp;= \left(\Sigma_p^{-1} + \Phi\frac{1}{\sigma_n^2}I\Phi^T\right)^{-1}\\
    &amp;= \left(\frac{1}{\sigma_n^2}\Phi\Phi^T + \Sigma_p^{-1}\right)^{-1}  \\
    &amp;= A^{-1}
\end{align}</span></p>
<p>Thus, we have the equivalent form for our predictive distribution: <span class="math display">\begin{equation}
    \boxed{p(y_*\vert \mathbf{x}_*, X, \mathbf{y}) =\\ \mathcal{N}\left( \phi_*^T\Sigma_p\Phi(K+\sigma_n^2I)^{-1}\mathbf{y}, \; \phi_*^T\Sigma_p\phi_* - \phi_*^T\Sigma_p\Phi(K+\sigma_n^2I)^{-1}\Phi^T\Sigma_p\phi_*\right)}
\end{equation}</span> where the pesky <span class="math inline">N\times N</span> term has been replaced by the <span class="math inline">n\times n</span> matrix <span class="math inline">\Phi^T\Sigma_p\Phi</span>.</p>
</section>
<section id="kernelization" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="kernelization">Kernelization</h2>
<p>We now make the the <em>key</em> observation that the only matrices that appear in the above expression are <span class="math display">\begin{align}
    &amp;\Phi^T\Sigma_p\Phi, &amp;\phi_*^T\Sigma_p\phi_* \\
    &amp;\phi_*^T\Sigma_p\Phi, &amp;\Phi^T\Sigma_p\phi_*
\end{align}</span> whose matrix elements we can write abstractly as <span class="math display">\begin{equation}
    \phi(\mathbf{x})^T\Sigma_p\phi(\mathbf{x}')
\end{equation}</span></p>
<p>To fit our model, we must determine appropriate values for the symmetric, positive semi-definite covariance matrix <span class="math inline">\Sigma_p</span> (and <span class="math inline">\sigma_n</span> too, technically). Instead, we observe that this matrix product is a quadratic form which we can think of as representing an inner product on our transformed vectors: <span class="math display">\begin{equation}
    K_{ij} = k(\mathbf{x}_i, \mathbf{x}_j) = \langle \phi(\mathbf{x}_i), \phi(\mathbf{x}_j)\rangle
\end{equation}</span> We call the function <span class="math inline">k(\mathbf{x},\mathbf{x}')</span> the <strong>kernel function</strong> or the <em>covariance function</em>.</p>
<div class="page-columns page-full"><p>All we need to perform the above calculations are the matrix elements of K on our data <span class="math inline">\mathcal{D}</span> and any test points <span class="math inline">\mathbf{x}_*</span> we wish to apply our model to. In effect, this means we are free to use feature vectors <a href="https://www.youtube.com/watch?v=XUj5JbQihlU&amp;t=25m53s"><strong>of any dimension, including <span class="math inline">\infty</span></strong></a>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;The idea here is that ther kernel vunction represents an inner product over <em>some</em> vector space. As it turns out, the RBF kernel corresponds to a an <a href="https://math.stackexchange.com/questions/276707/why-does-a-radial-basis-function-kernel-imply-an-infinite-dimension-map">infinite dimensional feature vector</a>.</p></li></div></div>
<p>There are many choices for the kernel function. One of the most popular is the RBF (radial basis function) kernel, also known as the <em>squared exponential kernel</em>:</p>
<p><span class="math display">\begin{equation}
    k_{\text{rbf}}(\mathbf{x}, \mathbf{x}') := \sigma_f^2\exp(-\frac{1}{2\ell^2}\lvert \mathbf{x}-\mathbf{x}'\rvert^2)
\end{equation}</span></p>
<p>where <span class="math inline">\sigma_f^2</span> is the <em>signal variance</em> and <span class="math inline">\ell</span> denotes the similarity length scale.</p>
<p>For notational convenience, let’s define <span class="math display">\begin{align}
    K &amp;:= k(X,X) \\
    K_{**} &amp;:= k(X_*, X_*) \\
    K_{*} &amp;:= k(X, X_*)
\end{align}</span></p>
<p>then, our predictive distribution takes the final, <em>clean</em> form <span class="math display">\begin{equation}
    \boxed{p(\mathbf{y}_* \vert X_*, X, \mathbf{y}) = \mathcal{N}\left( K_*^T(K+\sigma_n^2I)^{-1}\mathbf{y},\; K_{**}-K_{*}^T(K+\sigma_n^2I)^{-1}K_*\right)}
\end{equation}</span></p>
<p>This is the <em>end-result</em> of Gaussian Process Regression acheived via the <em>weight-space view</em>.</p>
</section>
</section>
<section id="the-function-space-view" class="level1">
<h1>The Function-space View</h1>
<p>So far our approach has been to generalize the standard linear regression model to allow for fitting over a (possibly infinite) basis of features <em>with</em> consideration for measurement and model uncertainty (our Bayesian priors). In essence, the idea was to fit <em>the distribution of all possible weights conditioned on the available training data</em>, <span class="math inline">p(\mathbf{w} \vert X, \mathbf{y})</span>. A second <em>equivalent</em> approach is to instead consider the distribution of all possible model function <span class="math inline">f(\mathbf{x})</span>. By constructing a Bayesian prior of this space, we constrain the space of possible model functions and fit a <em>distribution over all allowed model functions</em>, <span class="math inline">p(f \vert X, \mathbf{y})</span>. To do so we will need to develop the abstract machinery of distributions over function spaces. When these distributions are Gaussian in nature, the result is called a <em>Gaussian process</em>.</p>
<section id="gaussian-processes" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-processes">Gaussian Processes</h2>
<p>By this point, we are all familiar with the Gaussian distribution, aka the Normal distribtion <span class="math inline">\mathcal{N}(\mu, \sigma^2)</span>. This distribution is defined by a mean value <span class="math inline">\mu</span> and a variance <span class="math inline">\sigma^2</span>. It’s <em>big brother</em> is the <strong>Multivariate Normal Distribution</strong>, <span class="math inline">\mathcal{N}(\mathbf{\mu}, \Sigma)</span>, described be a vector of means <span class="math inline">\mathbf{\mu}}</span> and a covariance matrix <span class="math inline">\Sigma</span>. A natural question, then, is can we generalize the concept of the Gaussian distribution from <span class="math inline">N</span> dimensions to being defined over a continuous field? This question leads naturally to the definition of a <strong>Gaussian Process</strong></p>
<p><strong>Definition:</strong> A <em>Gaussian Process</em>, <span class="math inline">\mathcal{GP}</span>, is a collection of random variables for which any finite subset are described by a joint Gaussian distribution.</p>
<p>To see where this comes from, recall that in our previous derivation, we already made the assumption that all our our data points <span class="math inline">\mathcal{D}</span> are i.i.d. Gaussian distributed. A gaussian process is the natural extension of this and makes the assumption that the continuous set from which are data are sampled are <strong>so Guassian</strong> that any finite sample will be jointly Gaussian distributed. The term <em>process</em> is used to distinguish between finite collections of random variables (distributions) and their continuous counterparts described here.</p>
<p>Because each finite subset of this continuous collection is jointly gaussian, we can completely specify a Gaussian Process with two functions: the mean function <span class="math inline">m(\mathcal{x})</span> and the covariance function <span class="math inline">k(\mathbf{x},\mathbf{x}')</span>. To denote this, we typically write <span class="math display">\begin{equation}
    f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x},\mathbf{x}'))
\end{equation}</span></p>
</section>
<section id="bayesian-regression-is-a-gaussian-process" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-regression-is-a-gaussian-process">Bayesian Regression is a Gaussian Process</h2>
<p>To see this in action, recall our Bayesian regression model <span class="math display">\begin{equation}
    f(\mathbf{x} = \phi(\mathbf{x})^T\mathbf{w} \qquad \mathbf{w}\sim\mathcal{N}(\mathbf{0}, \Sigma_p)
\end{equation}</span> where we have set the prior on <span class="math inline">\mathcal{w}</span> to have zero mean.</p>
<p>The mean function is given by the expectation value of our model: <span class="math display">\begin{equation}
    \mathbb{E}[f(\mathbf{x})] = \phi(\mathbf{x})^T\mathbb{E}[\mathbf{w}] = 0
\end{equation}</span> and the covariance function is given by <span class="math display">\begin{equation}
\mathbb{E}[f(\mathbf{x})f(\mathbf{x'})] = \phi(\mathbf{x})^T\mathbb{E}[\mathbf{w}\mathbf{w}^T]\phi(\mathbf{x}') = \phi(\mathbf{x})^T\Sigma_p\phi(\mathbf{x}')
\end{equation}</span></p>
</section>
<section id="prediction-with-noise-free-observations" class="level2">
<h2 class="anchored" data-anchor-id="prediction-with-noise-free-observations">Prediction with Noise-free Observations</h2>
<p>To repeat the point, the key feature of Gaussian processes is that finite subsets are jointly Gaussian distributed. Thus we can we can split our data into the testpoints <span class="math inline">\mathcal{D}=(X,\mathbf{y})</span> and testpoints <span class="math inline">X_*</span> t and treat each collection as joint distributions with the following priors:</p>
<p><span class="math display">\begin{equation}\begin{bmatrix} \mathbf{f} \\ \mathbf{f}_* \end{bmatrix} \sim \mathcal{N}\left(\mathbf{0},\begin{bmatrix} K(X,X) &amp; K(X,X_*) \\ K(X_*,X) &amp; K(X_*,X_*) \end{bmatrix}\right)
\end{equation}</span></p>
<p>where <span class="math inline">\mathbf{f}:= f(X)</span> and <span class="math inline">\mathbf{f}_* = f(X_*)</span>.</p>
</section>
<section id="conditioning-the-joint-distribution" class="level2">
<h2 class="anchored" data-anchor-id="conditioning-the-joint-distribution">Conditioning the Joint Distribution</h2>
<p>To obtain our predictive distribution, <span class="math inline">p(\mathbf{f}_* \vert X_*, X, \mathbf{y})</span>, we <em>condition the joint prior distribution</em> on the observations. To see how this works, consider a general joint gaussian distribution given by <span class="math display">\begin{equation}
\begin{bmatrix} x \\ y \end{bmatrix} \sim \mathcal{N}\left( \begin{bmatrix}\mu_x \\ \mu_y\end{bmatrix},\; \begin{bmatrix} \Sigma_{xx} &amp; \Sigma_{xy} \\ \Sigma_{yx} &amp; \Sigma_{yy} \end{bmatrix}\right)
\end{equation}</span></p>
<p>define the centered values <span class="math inline">\tilde{x} := x-\mu_x</span> and <span class="math inline">\tilde{y} := x-\mu_y</span>. Define the intermediate variable <span class="math display">\begin{equation}
    z := \tilde{x} - A\tilde{y}
\end{equation}</span></p>
<p>Note that since we’ve subtracted out the mean we have <span class="math inline">\mathbb{E}[\tilde{x}] = \mathbb{E}[\tilde{y}] = \mathbb{E}[z] = 0</span></p>
<p>Let’s now find <span class="math inline">A</span>… <span class="math display">\begin{align}
    \mathbb{E}[z\tilde{y}^T] &amp;= \mathbb{E}[(\tilde{x}-A\tilde{y})\tilde{y}^T] \\
    &amp;= \mathbb{E}[\tilde{x}\tilde{y}^T - A\tilde{y}\tilde{y}] \\
    &amp;= \mathbb{E}[\tilde{x}\tilde{y}^T] - \mathbb{E}[A\tilde{y}\tilde{y}^T] \\
    &amp;= \Sigma_{xy} - A\mathbb{E}[\tilde{y}\tilde{y}^T] \\
    &amp;= \Sigma_{xy} - A\Sigma_{yy}
\end{align}</span></p>
<p>Therefore if we choose <span class="math inline">A</span> so that <span class="math inline">z</span> and <span class="math inline">\tilde{y}</span> are independent and uncorrelated, then <span class="math inline">\Sigma_{zy} = \mathbb{E}[z\tilde{y}^T] = 0</span>. Using this assumption, we find <span class="math display">\begin{equation}
    0 = \mathbb{E}[z\tilde{y}^T] = \Sigma_{xy}-A\Sigma_{yy} \\ \Rightarrow \boxed{A = \Sigma_{xy}\Sigma_{yy}^{-1}}
\end{equation}</span></p>
<p>If we now condition <span class="math inline">\tilde{x}</span> on <span class="math inline">\tilde{y}</span> (i.e.&nbsp;look at <span class="math inline">\tilde{x}</span> when <span class="math inline">\tilde{y}</span> is constant), we find <span class="math display">\begin{align}
    \mathbb{E}[\tilde{x}\vert\tilde{y}] &amp;= A\tilde{y} + \mathbb{E}[z] \\
    &amp;= A\tilde{y} + 0 \\
    &amp;= \Sigma_{xy}\Sigma_{yy}^{-1} \\
\end{align}</span></p>
<p>By manipulating this expression, we can now derive <span class="math inline">\mathbb{E}[x\vert y]</span> as follows: <span class="math display">\begin{align}
    \mathbb{E}[x\vert\tilde{y}] &amp;= \mathbb{E}[\tilde{x}\vert\tilde{y}] + \mu_x \\
    &amp;= \mu_x + \Sigma_{xy}\Sigma_{yy}^{-1}\tilde{y} \\
\end{align}</span> <span class="math display">\begin{equation}
\boxed{\mathbb{E}[x\vert y] = \mu_x + \Sigma_{xy}\Sigma_{yy}^{-1}(y-\mu_y)}
\end{equation}</span></p>
<p>Similarly for the covariance, we have <span class="math display">\begin{align}
    \text{Cov}(x \vert y) &amp;= \text{Cov}(\tilde{x}+\mu_x \vert \tilde{y}) \\
    &amp;= \text{Cov}(\tilde{x}+\mu_x \vert \tilde{y} + \mu_y) \\
    &amp;= \text{Cov}(\tilde{x}\vert(\tilde{y}+\mu_y)) \\
    &amp;= \text{Cov}(\tilde{x}\vert \tilde{y}) \\
    &amp;= \text{Cov}((z+A\tilde{y})\vert\tilde{y}) \\
    &amp;= \text{Cov}(z) + {A\text{Cov}(\tilde{y})} \\
    &amp;= \text{Cov}(z) + 0 \\
    &amp;= \mathbb{E}[zz^T] \\
    &amp;= \mathbb{E}[(\tilde{x}-A\tilde{y})(\tilde{x}-A\tilde{y})^T]\\
    &amp;= \mathbb{E}[\tilde{x}\tilde{x}^T - A\tilde{y}\tilde{x}^T -x(A\tilde{y})^T + A\tilde{y}\tilde{y}^TA^T] \\
    &amp;= \Sigma_{xx} - A\Sigma_{yx} - \Sigma_{xy}A^T + A\Sigma_{yy}A^T \\
    &amp;= \Sigma_{xx}-(\Sigma_{xy}\Sigma_{yy}^{-1})\Sigma_{yx} - \Sigma_{xy}(\Sigma_{yy}^{-1})^T\Sigma_{xy}^T + \Sigma_{xy}\Sigma_y^{-1}\Sigma_{y}(\Sigma_{y}^{-1})^T\Sigma_{xy}^T \\
    &amp;= \Sigma_{xx} - \Sigma_{xy}\Sigma{yy}^{-1}\Sigma_{xy}^T - \Sigma_{xy}(\Sigma_{yy}^{-1})^T\Sigma_{xy}^T + \Sigma_{xy}(\Sigma_{yy}^{-1})^T\Sigma_{xy}^T \\
    &amp;= \Sigma_{xx}-\Sigma_{xy}\left[\Sigma_{yy}^{-1} - (\Sigma_{yy}^{-1})^T + (\Sigma_{yy}^{-1})^T \right]\Sigma_{xy}^T \\
    &amp;= \Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}
\end{align}</span></p>
<p><span class="math display">\begin{equation}
\boxed{\text{Cov}(x \vert y) = \Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}}
\end{equation}</span></p>
<p>armed with this identity for joint Guassian distributions, we are ready to derive the predictive distribution for Gaussian Process Regression</p>
</section>
<section id="prediction-with-gaussian-processes" class="level2">
<h2 class="anchored" data-anchor-id="prediction-with-gaussian-processes">Prediction with Gaussian Processes</h2>
<p>Applying these results for our gaussian process, we find <span class="math display">\begin{equation}
    p(\mathbf{f}_* \vert X_*, X, \mathbf{y} = \mathcal{N}\left( K_*^TK^{-1}\mathbf{f},\; K_{**}-K_*^TK^{-1}K_*\right)
\end{equation}</span></p>
<p>To account for noisy observations, we can augment our correlation function to include a noise offset. The joint distrubtion then becomes:</p>
<p><span class="math display">\begin{equation}\begin{bmatrix} \mathbf{f} \\ \mathbf{f}_* \end{bmatrix} \sim \mathcal{N}\left(\mathbf{0},\begin{bmatrix} K(X,X)-\sigma_n^2I &amp; K(X,X_*) \\ K(X_*,X) &amp; K(X_*,X_*) \end{bmatrix}\right)
\end{equation}</span></p>
<p>which leads to the predictive distribution <span class="math display">\begin{equation}
    \boxed{p(\mathbf{f}_* \vert X_*, X, \mathbf{y}) = \mathcal{N}\left( K_*^T\left[K + \sigma_n^2 I\right]^{-1}\mathbf{f},\; K_{**}-K_*^T\left[K + \sigma_n^2 I\right]^{-1}K_*\right)}
\end{equation}</span></p>
</section>
</section>
<section id="doing-it-in-julia" class="level1">
<h1>Doing it in Julia</h1>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">LinearAlgebra</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Distributions</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Random</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Plots</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Tables </span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">MLJBase </span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">KernelFunctions </span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">AbstractGPs</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Statistics</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make training set</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">make_training_data</span>(n, ν)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> <span class="fu">reshape</span>(<span class="fu">collect</span>(<span class="fu">range</span>(<span class="fl">0.0</span>, stop<span class="op">=</span><span class="fl">1.0</span>, length<span class="op">=</span><span class="fl">500</span>)), (<span class="fl">1</span>,<span class="fl">500</span>))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">y</span>(x) <span class="op">=</span> (<span class="fu">exp</span>(<span class="op">-</span>x<span class="op">/</span>(<span class="fl">0.5</span>)<span class="op">^</span><span class="fl">2</span>) <span class="op">*</span> <span class="fu">sin</span>(<span class="fl">2</span>π<span class="op">*</span>ν<span class="op">*</span>x)) <span class="op">+</span> (<span class="fl">0.3</span>)<span class="op">^</span><span class="fl">2</span><span class="fu">*</span>(<span class="fu">rand</span>()<span class="op">-</span><span class="fl">0.5</span>)  <span class="co"># this one has some noise</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ytruth</span>(x) <span class="op">=</span> (<span class="fu">exp</span>(<span class="op">-</span>x<span class="op">/</span>(<span class="fl">0.5</span>)<span class="op">^</span><span class="fl">2</span>) <span class="op">*</span> <span class="fu">sin</span>(<span class="fl">2</span>π<span class="op">*</span>ν<span class="op">*</span>x)) </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> Tables.<span class="fu">table</span>(<span class="fu">rand</span>(<span class="fl">1</span>,n)<span class="ch">', header=[:x])</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    Xtrue <span class="op">=</span> Tables.<span class="fu">table</span>(xs<span class="op">'</span>, header<span class="op">=</span>[<span class="op">:</span>x])</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="fu">y</span>.(X.x)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    ytrue <span class="op">=</span> <span class="fu">ytruth</span>.(Xtrue.x)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y, Xtrue, ytrue</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>X, y, Xtrue, ytrue <span class="op">=</span> <span class="fu">make_training_data</span>(<span class="fl">50</span>, <span class="fl">10</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="fu">scatter</span>(X.x, y, color<span class="op">=:</span>red, label<span class="op">=</span><span class="st">"noisy data"</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(Xtrue.x, ytrue, color<span class="op">=:</span>blue, label<span class="op">=</span><span class="st">"true function"</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="fu">xlabel!</span>(<span class="st">"x"</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="fu">ylabel!</span>(<span class="st">"y"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<p><img src="gaussian_process_regression_overview_files/figure-html/cell-7-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p><a href="https://juliagaussianprocesses.github.io/KernelFunctions.jl/dev/userguide/"><code>KernelFunctions.jl</code></a> provides a clean interface to create various kernelfunctions and apply them to data to create our matrices <code>K</code>.</p>
<p>Due to the fact that kernel functions obey composition laws, we can easily build up complicated Kernels from basic pieces via function composition with <span class="math inline">\circ</span></p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create kernel function</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>ℓ <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># length scale parameter</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="fl">1.0</span>  <span class="op">*</span> (<span class="fu">SqExponentialKernel</span>() <span class="op">∘</span> <span class="fu">ScaleTransform</span>(<span class="fl">1</span><span class="op">/</span><span class="fl">2</span>ℓ<span class="op">^</span><span class="fl">2</span>))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># test out kernels interface by generating matrix from k</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="fu">kernelmatrix</span>(k, Tables.<span class="fu">matrix</span>(X)<span class="ch">')  # transpose as we want size (n_features, n_datapoints)</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="fu">heatmap</span>(K, yflip<span class="op">=</span><span class="cn">true</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<p><img src="gaussian_process_regression_overview_files/figure-html/cell-8-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>Unsurprisingly, there is a lot of activation on the diagonal as for a single datapoint <span class="math inline">\mathbf{x}</span>, we have <span class="math display">\begin{equation}
    k(\mathbf{x},\mathbf{x}) = \exp\left(-\frac{0}{2\ell^2} \right) = 1.0
\end{equation}</span></p>
<p>Now that we have our Kernel function, let’s construct our Gaussian Process.</p>
<p><a href="https://juliagaussianprocesses.github.io/AbstractGPs.jl/dev/"><code>AbstractGPs.jl</code></a> provides an excellent way to define Gaussian Processes by supplying mean and kernel functions. We can then sample from our GPs with a simple interface designed to extend the basic functions from <code>Statistics.jl</code>. From an <code>AbstractGP</code> we can construct a <code>FiniteGP</code> by <em>indexing</em> into our datasets.</p>
<p>First we construct <span class="math inline">f\sim\mathcal{GP}(0, k(\cdot, \cdot))</span></p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># construct an AbstractGP with mean 0 and cov k(x,x')</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="fu">GP</span>(<span class="fl">0</span>, k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>GP{AbstractGPs.ConstMean{Int64}, ScaledKernel{TransformedKernel{SqExponentialKernel{Distances.Euclidean}, ScaleTransform{Float64}}, Float64}}(AbstractGPs.ConstMean{Int64}(0), Squared Exponential Kernel (metric = Distances.Euclidean(0.0))
    - Scale Transform (s = 49.99999999999999)
    - σ² = 1.0)</code></pre>
</div>
</div>
<p>From this <code>AbstractGP</code>, we can now construct a <em>FiniteGP</em>, i.e.&nbsp;a multivariate normal distribution by applying GP to our training data. We include a measurement variance of <span class="math inline">\sigma^2 = 0.1</span> to account for noisy observations</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>fₓ <span class="op">=</span> <span class="fu">f</span>(Tables.<span class="fu">matrix</span>(X)<span class="ch">', (0.1)^2)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>AbstractGPs.FiniteGP{GP{AbstractGPs.ConstMean{Int64}, ScaledKernel{TransformedKernel{SqExponentialKernel{Distances.Euclidean}, ScaleTransform{Float64}}, Float64}}, ColVecs{Float64, Matrix{Float64}, SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}}, Diagonal{Float64, FillArrays.Fill{Float64, 1, Tuple{Base.OneTo{Int64}}}}}(
f: GP{AbstractGPs.ConstMean{Int64}, ScaledKernel{TransformedKernel{SqExponentialKernel{Distances.Euclidean}, ScaleTransform{Float64}}, Float64}}(AbstractGPs.ConstMean{Int64}(0), Squared Exponential Kernel (metric = Distances.Euclidean(0.0))
    - Scale Transform (s = 49.99999999999999)
    - σ² = 1.0)
x: SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}[[0.5787278383554891], [0.9142048469151632], [0.6349481630131082], [0.09171915630391003], [0.7335198108794088], [0.47760835807275903], [0.02449415371328323], [0.6044400921590919], [0.5943832380403392], [0.31057576909916196]  …  [0.913051019929006], [0.5985706916405367], [0.7065005588747734], [0.5987990720193792], [0.26702829949066875], [0.4534173721134228], [0.2656832707543302], [0.5016867800937572], [0.7817138865001715], [0.6118324809053808]]
Σy: [0.010000000000000002 0.0 … 0.0 0.0; 0.0 0.010000000000000002 … 0.0 0.0; … ; 0.0 0.0 … 0.010000000000000002 0.0; 0.0 0.0 … 0.0 0.010000000000000002]
)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's examine this object </span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="fu">fieldnames</span>(<span class="fu">typeof</span>(fₓ)))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>fₓ.x.X  <span class="co"># the underlying data </span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>fₓ.x    <span class="co"># internel data representation as columns for fast computation of kernel matrix</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>fₓ.Σy   <span class="co"># Data Covariance matrix constructed from the σ² we supplied. Note the sparsity</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(:f, :x, :Σy)</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>50×50 Diagonal{Float64, FillArrays.Fill{Float64, 1, Tuple{Base.OneTo{Int64}}}}:
 0.01   ⋅     ⋅     ⋅     ⋅     ⋅    …   ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅    0.01   ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅    0.01   ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅    0.01   ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅    0.01   ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅    0.01  …   ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅    …   ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
 ⋮                             ⋮     ⋱        ⋮                       
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅    …   ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅       0.01   ⋅     ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅    …   ⋅    0.01   ⋅     ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅    0.01   ⋅     ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅    0.01   ⋅     ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅    0.01   ⋅ 
  ⋅     ⋅     ⋅     ⋅     ⋅     ⋅        ⋅     ⋅     ⋅     ⋅     ⋅    0.01</code></pre>
</div>
</div>
<p>Now that we have our Gaussian Process, we can compute the log-marginal likelihood <span class="math inline">p(\mathbf{y}\vert X, \theta)</span>, i.e.&nbsp;the probabity of obtaining the targets given the features, hyperparameters, etc…</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">logpdf</span>(fₓ, y)  <span class="co"># this is a marginal likelihood</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>-7.700184939882172</code></pre>
</div>
</div>
<p>Next, we demonstrate how to compute the posterior Gaussian process (for us that would be <span class="math inline">f_*</span>). First we create the finite gaussian process (a function) which we will use to compute the posterior distribution <span class="math display">\begin{equation}
    p(\mathbf{f}_* \vert X_*, X, \mathbf{y})
\end{equation}</span></p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>p_fₓ <span class="op">=</span> <span class="fu">posterior</span>(fₓ, y)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="fu">typeof</span>(p_fₓ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>AbstractGPs.PosteriorGP{GP{AbstractGPs.ConstMean{Int64}, ScaledKernel{TransformedKernel{SqExponentialKernel{Distances.Euclidean}, ScaleTransform{Float64}}, Float64}}, NamedTuple{(:α, :C, :x, :δ), Tuple{Vector{Float64}, Cholesky{Float64, Matrix{Float64}}, ColVecs{Float64, Matrix{Float64}, SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}}, Vector{Float64}}}}</code></pre>
</div>
</div>
<p>Now that we have the distribution, we can form our predictions… This can be done a few different ways:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>fₓ_pred <span class="op">=</span> <span class="fu">p_fₓ</span>(Tables.<span class="fu">matrix</span>(Xtrue)<span class="ch">')</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>ypred <span class="op">=</span> <span class="fu">mean</span>(fₓ_pred)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>yσ² <span class="op">=</span> <span class="fu">var</span>(fₓ_pred)  <span class="co"># variance is diagonal of resulting covariance matrix</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>500-element Vector{Float64}:
 0.7399969405698197
 0.6745544116473559
 0.6012053216016902
 0.5216117222468406
 0.4382010286577165
 0.3540638974650211
 0.2727409320138179
 0.19790673960059169
 0.1329806849891212
 0.08071357072269503
 0.042813732967400764
 0.019680168275069354
 0.010301206805772735
 ⋮
 0.03407265166349216
 0.05774728658904227
 0.09110947387388468
 0.13471067956710525
 0.18827073610317135
 0.25067899726970433
 0.3201038052855034
 0.39418761362428856
 0.47029407824696645
 0.5457689535207053
 0.6181787610714443
 0.685498828645858</code></pre>
</div>
</div>
<p>Alternatively, if we instead want a distribution for each datapoint we can compute <span class="math display">\begin{equation}
    p(\mathbf{y}_x \vert \mathbf{x}_*, X, y)
\end{equation}</span> When treated as a collection, we can think about each of these representing a marginalized distribution over the test points <span class="math inline">\mathbf{x}_*</span> and hence, we call <code>marginals()</code></p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>p_pred <span class="op">=</span> <span class="fu">marginals</span>(fₓ_pred)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(p_pred[<span class="fl">1</span>])</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> <span class="fu">mean</span>.(p_pred);</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>y_σ <span class="op">=</span> <span class="fu">std</span>.(p_pred);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Normal{Float64}(μ=0.45719064373766344, σ=0.8602307484447529)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="op">=</span> <span class="fu">plot</span>(X.x, y, seriestype<span class="op">=:</span>scatter, color<span class="op">=:</span>red, label<span class="op">=</span><span class="st">"noisy data"</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(Xtrue.x, ytrue, color<span class="op">=:</span>black, linestyle<span class="op">=:</span>dash, label<span class="op">=</span><span class="st">"true function"</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(Xtrue.x, y_pred, color<span class="op">=:</span>blue, linewidth<span class="op">=</span><span class="fl">3</span>, label<span class="op">=</span><span class="st">"fit"</span> )</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(Xtrue.x, y_pred <span class="op">.+</span> <span class="fl">2</span>y_σ, c<span class="op">=:</span>gray, label<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(Xtrue.x, y_pred <span class="op">.-</span> <span class="fl">2</span>y_σ, fillrange <span class="op">=</span> y_pred <span class="op">.+</span> <span class="fl">2</span>y_σ, fillalpha<span class="op">=</span><span class="fl">0.25</span>, color<span class="op">=:</span>gray, label<span class="op">=</span><span class="st">"± 2σ"</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="fu">xlabel!</span>(<span class="st">"x"</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ylabel!</span>(<span class="st">"y"</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="fu">title!</span>(<span class="st">"Vanilla GPR"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<p><img src="gaussian_process_regression_overview_files/figure-html/cell-16-output-1.svg" class="img-fluid"></p>
</div>
</div>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<p>So far we have shown how to:</p>
<ol type="1">
<li>Build a kernel function <span class="math inline">k(\cdot, \cdot)</span> via composition using <code>KernelFunctions.jl</code></li>
<li>Construct an a Gaussian Process <span class="math inline">f\sim\mathcal{GP}</span> abstractly using <code>AbstractGPs.jl</code></li>
<li>Construct a finite representation of our GP, <span class="math inline">f_x</span>, over training data</li>
<li>Construct a posterior Gaussian Process from <span class="math inline">f_x</span> and our training targets <span class="math inline">\mathbf{y}</span>.</li>
<li>Construct a finite representation of the posterior GP applied to our prediction data (here <code>Xtrue</code>).</li>
<li>Sample this final distribution to obatin a prediction via <code>mean()</code> and variances via <code>var()</code>. Alternatively, we can obtain a multivariate normal distribution for each point by calling <code>marginals()</code>.</li>
</ol>
</section>
</section>
<section id="fitting-the-gaussian-process" class="level1">
<h1>Fitting the Gaussian Process</h1>
<p>You may think we have <em>already</em> fit the Guassian process however, we were forced to choose values for both <span class="math inline">\ell</span> and <span class="math inline">\sigma^2</span>. How can we optimally select the ideal hyperparameters for our Gaussian Process? This leads us into the realm of <a href="https://gaussianprocess.org/gpml/chapters/RW5.pdf">Bayesian Model Selection</a></p>
<section id="bayesian-model-selection" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-model-selection">Bayesian Model Selection</h2>
<p>There are several levels of parameters in machine learning. At the lowest level, we have the model weights <span class="math inline">\mathbf{w}</span>. Above that, we have model hyperparameters, <span class="math inline">\theta</span>. At the top we have model structure <span class="math inline">\mathcal{H}</span>. In our Bayesian framework, we can consider distributions defined at each of these levels. At the bottom, we have <span class="math display">\begin{equation}
    p(\mathbf{w} \vert X, \mathbf{y}, \theta, \mathcal{H}_i) = \frac{p(\mathbf{y} \vert X, \mathbf{w}, \theta, \mathcal{H}_i) p(\mathbf{w}\vert \theta, \mathcal{H}_i) }{p(\mathbf{y}\vert X, \theta, \mathcal{H}_i)}
\end{equation}</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If this looks confusing, consider Bayes rule for 3 events <span class="math inline">R, H, S</span>. We have: <span class="math display">\begin{align}
    P(R \vert H, S) &amp;= \frac{P(R,H,S)}{P(H,S)} \\
    &amp;= \frac{P(H \vert R, S)P(R, S)}{P(H,S)}\\
    &amp;= \frac{P(H \vert R, S)P(R\vert S)P(S)}{P(H\vert S)P(S)} \\
    &amp;= \frac{P(H \vert R, S)P(R\vert S)}{P(H\vert S)}
\end{align}</span></p>
<p>To get the result, just think of <span class="math inline">\theta</span> and <span class="math inline">\mathcal{H}_i</span> as a single <em>event</em> and translate the above to distribution functions.</p>
<p><a href="https://math.stackexchange.com/questions/1281454/bayes-rule-with-3-variables">stack exchange link</a></p>
</div>
</div>
</div>
<p>The prior <span class="math inline">p(\mathbf{w}\vert \theta, \mathcal{H}_i)</span> encodes any knowledge we have about the parameters prior to seeing the data. The denominator is the <em>marginal likelihood</em> and is given by <span class="math display">\begin{equation}
    p(\mathbf{y}\vert X, \theta, \mathcal{H}_i) = \int d\mathbf{w}\; p(\mathbf{y} \vert X, \mathbf{w}, \theta, \mathcal{H}_i)p(\mathbf{w}\vert \theta, \mathcal{H}_i)
\end{equation}</span></p>
<p>The next level up is to express the distribution of hyper-parameters <span class="math inline">\theta</span>: <span class="math display">\begin{equation}
    p(\theta \vert X, \mathbf{y}, \mathcal{H}_i) = \frac{p(\mathbf{y}\vert X, \theta, \mathcal{H}_i)p(\theta \vert \mathcal{H}_i)}{p(\mathbf{y}\vert X, \mathcal{H}_i)}
\end{equation}</span> Here <span class="math inline">p(\theta \vert \mathcal{H}_i)</span> is called the <em>hyper-prior</em>. Similarly, the normalization constant is given by <span class="math display">\begin{equation}
    p(\mathbf{y}\vert X,\mathcal{H}_i) = \int d\theta \; p(\mathbf{y}\vert X, \theta, \mathcal{H}_i)p(\theta \vert \mathcal{H}_i)
\end{equation}</span></p>
<p>Finally, at the top level we have the set of possible model structures <span class="math inline">\{\mathcal{H}_i\}</span>. This leads to <span class="math display">\begin{equation}
    p(\mathcal{H}_i \vert X, \mathbf{y}) = \frac{p(\mathbf{y} \vert X, \mathcal{H}_i)p(\mathcal{H}_i)}{p(\mathbf{y}\vert X)}
\end{equation}</span> with normlization constant <span class="math display">\begin{equation}
p(\mathbf{y}\vert X) = \sum_i p(\mathbf{y} \vert X, \mathcal{H}_i)p(\mathcal{H}_i)
\end{equation}</span></p>
<p>Depending on the model details, these integrals may be intractible to approximations or Monte Carlo methods. Since we rarely have sufficient knowledge to form a hyperparameter prior, one often attempts to maximize the marginal likelihood <span class="math inline">p(\mathbf{y} \vert X, \theta, \mathcal{H}_i)</span> with respect to the hyperparameters <span class="math inline">\theta</span> instead. This is known as Type II Maximium Likelihood Estimation.</p>
<p>In the case of Gaussian Process Regression, we are once again saved by the fact that every piece has a convenient functional from resulting in analytically tractible integrals for the marginal likelihood function. We find <span class="math display">\begin{equation}
    \ln p(\mathbf{y}\vert X, \theta) = -\frac{1}{2}\mathbf{y}^T(K_f + \sigma_n^2 I)^{-1}\mathbf{y} - \frac{1}{2}\ln\lvert K_f + \sigma_n^2 I \rvert -\frac{n}{2}\ln(2\pi)
\end{equation}</span></p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We should add a derivation of this when possible. It’s just a big ’ol nasty integral.</p>
</div>
</div>
</div>
<p>Let’s try this out in code! See <a href="https://juliagaussianprocesses.github.io/AbstractGPs.jl/dev/examples/0-intro-1d/#Exact-Gaussian-Process-Inference">this section</a> from the <code>AbstractGPs.jl</code> docs…</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Optim</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># helpuer functions to insure parameters are positive</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">softplus</span>(x) <span class="op">=</span> <span class="fu">log</span>(<span class="fl">1</span><span class="fu">+exp</span>(x))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">softplusinv</span>(x) <span class="op">=</span> <span class="fu">log</span>(<span class="fu">exp</span>(x)<span class="op">-</span><span class="fl">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="96">
<pre><code>softplusinv (generic function with 1 method)</code></pre>
</div>
</div>
<p>We want to maximize the log-marginal-likelihood and therefore want to minimize minus that quantitty:</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define loss function</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">loss_function</span>(x,y)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">function</span> <span class="fu">negativelogmarginallikelihood</span>(params)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>        kernel <span class="op">=</span> <span class="fu">softplus</span>(params[<span class="fl">1</span>]) <span class="op">*</span> (<span class="fu">SqExponentialKernel</span>() <span class="op">∘</span> <span class="fu">ScaleTransform</span>(<span class="fu">softplus</span>(params[<span class="fl">2</span>])))</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>         f <span class="op">=</span> <span class="fu">GP</span>(kernel)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        fx <span class="op">=</span> <span class="fu">f</span>(x, <span class="fu">softplus</span>(params[<span class="fl">3</span>]))</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fu">-logpdf</span>(fx, y)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">end</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> negativelogmarginallikelihood</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>loss_function (generic function with 1 method)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prefactor, length parameter, measurement variance</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>θ₀ <span class="op">=</span> [<span class="fl">1.0</span>, <span class="fl">1</span><span class="op">/</span>(<span class="fl">2</span>ℓ<span class="op">^</span><span class="fl">2</span>), (<span class="fl">0.25</span>)<span class="op">^</span><span class="fl">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>3-element Vector{Float64}:
  1.0
 49.99999999999999
  0.0625</code></pre>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># default option uses finite diff methods</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> Optim.<span class="fu">optimize</span>(<span class="fu">loss_function</span>(Tables.<span class="fu">matrix</span>(X)<span class="ch">', y), θ₀, LBFGS(); autodiff=:forward)  </span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>params_best <span class="op">=</span> opt.minimizer</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="fu">softplus</span>.(params_best)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>3-element Vector{Float64}:
  0.1206544051219394
 37.409897810113726
  0.000582519141662673</code></pre>
</div>
</div>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>kernel_best <span class="op">=</span> <span class="fu">softplus</span>(params_best[<span class="fl">1</span>]) <span class="op">*</span> (<span class="fu">SqExponentialKernel</span>() <span class="op">∘</span> <span class="fu">ScaleTransform</span>(<span class="fu">softplus</span>(params_best[<span class="fl">2</span>])))</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="fu">GP</span>(kernel_best)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>fₓ <span class="op">=</span> <span class="fu">f</span>(Tables.<span class="fu">matrix</span>(X)<span class="ch">', softplus(params_best[3]))</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>p_fₓ <span class="op">=</span> <span class="fu">posterior</span>(fₓ, y)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>fₓ_pred <span class="op">=</span> <span class="fu">p_fₓ</span>(Tables.<span class="fu">matrix</span>(Xtrue)<span class="ch">')</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>p_pred <span class="op">=</span> <span class="fu">marginals</span>(fₓ_pred)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> <span class="fu">mean</span>.(p_pred);</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>y_σ <span class="op">=</span> <span class="fu">std</span>.(p_pred);</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>p2 <span class="op">=</span> <span class="fu">plot</span>(X.x, y, seriestype<span class="op">=:</span>scatter, color<span class="op">=:</span>red, label<span class="op">=</span><span class="st">"noisy data"</span>)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(Xtrue.x, ytrue, color<span class="op">=:</span>black, linestyle<span class="op">=:</span>dash, label<span class="op">=</span><span class="st">"true function"</span>)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(Xtrue.x, y_pred, color<span class="op">=:</span>blue, linewidth<span class="op">=</span><span class="fl">3</span>, label<span class="op">=</span><span class="st">"fit"</span> )</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(Xtrue.x, y_pred <span class="op">.+</span> <span class="fl">2</span>y_σ, c<span class="op">=:</span>gray, label<span class="op">=</span><span class="st">""</span>)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot!</span>(Xtrue.x, y_pred <span class="op">.-</span> <span class="fl">2</span>y_σ, fillrange <span class="op">=</span> y_pred <span class="op">.+</span> <span class="fl">2</span>y_σ, fillalpha<span class="op">=</span><span class="fl">0.25</span>, color<span class="op">=:</span>gray, label<span class="op">=</span><span class="st">"± 2σ"</span>)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="fu">xlabel!</span>(<span class="st">"x"</span>)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="fu">ylabel!</span>(<span class="st">"y"</span>)</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="fu">title!</span>(<span class="st">"HPO GPR"</span>)</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p1, p2, layout<span class="op">=</span>(<span class="fl">1</span>,<span class="fl">2</span>), size<span class="op">=</span>(<span class="fl">900</span>, <span class="fl">500</span>), plot_title<span class="op">=</span><span class="st">"Gaussian Process Regression Fits"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<p><img src="gaussian_process_regression_overview_files/figure-html/cell-22-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>Excellent! Now that we have the hang of it, let’s try another fit for a function of two variables.</p>
<div class="cell" data-execution_count="157">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="fl">1000</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> (<span class="fu">rand</span>(n,<span class="fl">2</span>) <span class="op">.-</span> <span class="fl">0.5</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>x₁ <span class="op">=</span> xs[<span class="op">:</span>,<span class="fl">1</span>]</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>x₂ <span class="op">=</span> xs[<span class="op">:</span>,<span class="fl">2</span>]</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x₁<span class="op">.^</span><span class="fl">5</span> <span class="op">.+</span> x₂<span class="op">.^</span><span class="fl">4</span> <span class="op">.-</span> x₁<span class="op">.^</span><span class="fl">4</span> <span class="op">.-</span> x₂<span class="op">.^</span><span class="fl">3</span> </span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>Xtrain <span class="op">=</span> Tables.<span class="fu">table</span>(xs[<span class="fl">1</span><span class="op">:</span><span class="fu">Int</span>(<span class="fl">0.9</span><span class="op">*</span>n), <span class="op">:</span>], header<span class="op">=</span>[<span class="op">:</span>x₁, <span class="op">:</span>x₂])</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>Xtest <span class="op">=</span> Tables.<span class="fu">table</span>(xs[<span class="fu">Int</span>(<span class="fl">0.9</span><span class="op">*</span>n)<span class="op">+</span><span class="fl">1</span><span class="op">:</span><span class="kw">end</span>, <span class="op">:</span>], header<span class="op">=</span>[<span class="op">:</span>x₁, <span class="op">:</span>x₂])</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>ytrain <span class="op">=</span> y[<span class="fl">1</span><span class="op">:</span><span class="fu">Int</span>(<span class="fl">0.9</span><span class="op">*</span>n)]</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>ytest <span class="op">=</span> y[<span class="fu">Int</span>(<span class="fl">0.9</span><span class="op">*</span>n)<span class="op">+</span><span class="fl">1</span><span class="op">:</span><span class="kw">end</span>]</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="fu">scatter</span>(Xtrain.x₁, Xtrain.x₂, ytrain, ms<span class="op">=</span><span class="fl">3</span>, label<span class="op">=</span><span class="st">"training points"</span>)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="fu">scatter!</span>(Xtest.x₁, Xtest.x₂, ytest, color<span class="op">=:</span>red, ms<span class="op">=</span><span class="fl">3</span>, label<span class="op">=</span><span class="st">"testing points"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="157">
<p><img src="gaussian_process_regression_overview_files/figure-html/cell-23-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="162">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this is me just trying to manuallyfind some good parameters</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>σf² <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>ℓ <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>σn² <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> σf²  <span class="op">*</span> (<span class="fu">SqExponentialKernel</span>() <span class="op">∘</span> <span class="fu">ScaleTransform</span>(<span class="fl">1</span><span class="op">/</span><span class="fl">2</span>ℓ<span class="op">^</span><span class="fl">2</span>))</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># test out kernels interface by generating matrix from k</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="fu">kernelmatrix</span>(k, Tables.<span class="fu">matrix</span>(Xtrain)<span class="ch">')  # transpose as we want size (n_features, n_datapoints)</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="fu">GP</span>(<span class="fl">0</span>, k)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>fₓ <span class="op">=</span> <span class="fu">f</span>(Tables.<span class="fu">matrix</span>(Xtrain)<span class="ch">', σn²)</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="fu">logpdf</span>(fₓ, ytrain)  <span class="co"># this is a marginal likelihood</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>p_fₓ <span class="op">=</span> <span class="fu">posterior</span>(fₓ, ytrain)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>fₓ_pred_train <span class="op">=</span> <span class="fu">p_fₓ</span>(Tables.<span class="fu">matrix</span>(Xtrain)<span class="ch">')</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>ypred_train <span class="op">=</span> <span class="fu">mean</span>(fₓ_pred_train)</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>yσ²_train <span class="op">=</span> <span class="fu">var</span>(fₓ_pred_train)  <span class="co"># variance is diagonal of resulting covariance matrix</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>fₓ_pred_test <span class="op">=</span> <span class="fu">p_fₓ</span>(Tables.<span class="fu">matrix</span>(Xtest)<span class="ch">')</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>ypred_test <span class="op">=</span> <span class="fu">mean</span>(fₓ_pred_test)</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>yσ²_test <span class="op">=</span> <span class="fu">var</span>(fₓ_pred_test)  <span class="co"># variance is diagonal of resulting covariance matrix</span></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>pf1 <span class="op">=</span> <span class="fu">scatter</span>(ypred_train, ytrain, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a><span class="fu">scatter!</span>(ypred_test, ytest, label<span class="op">=</span><span class="st">"testing data"</span>)</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a><span class="fu">title!</span>(<span class="st">"Fit with guessed parameters"</span>)</span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="162">
<p><img src="gaussian_process_regression_overview_files/figure-html/cell-24-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>we need a way to intelligently initialize the outputs. See <a href="https://infallible-thompson-49de36.netlify.app/">this post</a> for ideas</p>
<div class="cell" data-execution_count="163">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>θ_lower <span class="op">=</span> [<span class="fl">1e-5</span>, <span class="fl">1e-5</span>, <span class="fl">1e-5</span>]</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>θ_upper <span class="op">=</span> [<span class="cn">Inf</span>, <span class="cn">Inf</span>, <span class="cn">Inf</span>]</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>θ₀ <span class="op">=</span> [<span class="fl">0.25</span>, <span class="fl">0.3</span>, <span class="fl">0.25</span>]</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># define loss function</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">loss_function</span>(x,y)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">function</span> <span class="fu">negativelogmarginallikelihood</span>(params)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>        kernel <span class="op">=</span> params[<span class="fl">1</span>] <span class="op">*</span> (<span class="fu">SqExponentialKernel</span>() <span class="op">∘</span> <span class="fu">ScaleTransform</span>(params[<span class="fl">2</span>]))</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>         f <span class="op">=</span> <span class="fu">GP</span>(kernel)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>        fx <span class="op">=</span> <span class="fu">f</span>(x, params[<span class="fl">3</span>])</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fu">-logpdf</span>(fx, y)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">end</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> negativelogmarginallikelihood</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a><span class="co"># default option uses finite diff methods</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> Optim.<span class="fu">optimize</span>(<span class="fu">loss_function</span>(Tables.<span class="fu">matrix</span>(Xtrain)<span class="ch">', ytrain), θ_lower, θ_upper, θ₀, Fminbox(LBFGS()); autodiff=:forward)  </span></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>params_best <span class="op">=</span> opt.minimizer</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a><span class="co"># softplus.(params_best)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="163">
<pre><code>3-element Vector{Float64}:
 7.44878713782739
 1.062150755934373
 1.0000000000000003e-5</code></pre>
</div>
</div>
<div class="cell" data-execution_count="165">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>kernel_best <span class="op">=</span> params_best[<span class="fl">1</span>] <span class="op">*</span> (<span class="fu">SqExponentialKernel</span>() <span class="op">∘</span> <span class="fu">ScaleTransform</span>(params_best[<span class="fl">2</span>]))</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="fu">GP</span>(kernel_best)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>fₓ <span class="op">=</span> <span class="fu">f</span>(Tables.<span class="fu">matrix</span>(Xtrain)<span class="ch">', params_best[3])</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>p_fₓ <span class="op">=</span> <span class="fu">posterior</span>(fₓ, ytrain)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co"># compute </span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>fₓ_pred <span class="op">=</span> <span class="fu">p_fₓ</span>(Tables.<span class="fu">matrix</span>(Xtest)<span class="ch">')</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>p_pred <span class="op">=</span> <span class="fu">marginals</span>(fₓ_pred)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> <span class="fu">mean</span>.(p_pred);</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>y_σ <span class="op">=</span> <span class="fu">std</span>.(p_pred);</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>fₓ_pred_train <span class="op">=</span> <span class="fu">p_fₓ</span>(Tables.<span class="fu">matrix</span>(Xtrain)<span class="ch">')</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>ypred_train <span class="op">=</span> <span class="fu">mean</span>(fₓ_pred_train)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>yσ²_train <span class="op">=</span> <span class="fu">var</span>(fₓ_pred_train)  <span class="co"># variance is diagonal of resulting covariance matrix</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>fₓ_pred_test <span class="op">=</span> <span class="fu">p_fₓ</span>(Tables.<span class="fu">matrix</span>(Xtest)<span class="ch">')</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>ypred_test <span class="op">=</span> <span class="fu">mean</span>(fₓ_pred_test)</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>yσ²_test <span class="op">=</span> <span class="fu">var</span>(fₓ_pred_test)  <span class="co"># variance is diagonal of resulting covariance matrix</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>pf2 <span class="op">=</span> <span class="fu">scatter</span>(ypred_train, ytrain, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a><span class="fu">scatter!</span>(ypred_test, ytest, label<span class="op">=</span><span class="st">"testing data"</span>)</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a><span class="fu">title!</span>(<span class="st">"Fit with HPO Parameters"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="165">
<p><img src="gaussian_process_regression_overview_files/figure-html/cell-26-output-1.svg" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="175">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">marginals</span>(fₓ_pred_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="175">
<pre><code>900-element Vector{Normal{Float64}}:
 Normal{Float64}(μ=0.010400795687473874, σ=0.00035037346174344536)
 Normal{Float64}(μ=-0.012172544294344334, σ=0.0003825535966581155)
 Normal{Float64}(μ=-0.003692183680868766, σ=0.00036326742814205324)
 Normal{Float64}(μ=0.0030344804249580193, σ=0.00035299693729260947)
 Normal{Float64}(μ=-0.008193057695734751, σ=0.0003798955083331059)
 Normal{Float64}(μ=0.15378105970739853, σ=0.0008182225299438748)
 Normal{Float64}(μ=-0.038238358903527114, σ=0.0005019506938365845)
 Normal{Float64}(μ=-0.05854285968462136, σ=0.0007776923621838775)
 Normal{Float64}(μ=-0.01249912826324362, σ=0.0003932095861878094)
 Normal{Float64}(μ=0.057902235075289354, σ=0.00041030597882028684)
 Normal{Float64}(μ=0.0011632400992311887, σ=0.00034108646783589946)
 Normal{Float64}(μ=-0.018200111100668437, σ=0.0004837689169881995)
 Normal{Float64}(μ=-0.002522870589473314, σ=0.0003555895502088072)
 ⋮
 Normal{Float64}(μ=-0.05043878931292056, σ=0.0004920300553927884)
 Normal{Float64}(μ=-0.016010720361464337, σ=0.00040222272797466903)
 Normal{Float64}(μ=0.06253937610745197, σ=0.0007782123676896544)
 Normal{Float64}(μ=0.04114382454235965, σ=0.000392768595621182)
 Normal{Float64}(μ=0.0014618399472965393, σ=0.00035841328830948124)
 Normal{Float64}(μ=-0.0014261831347539555, σ=0.00035119866114851093)
 Normal{Float64}(μ=-0.04270731098517899, σ=0.0005029644552482196)
 Normal{Float64}(μ=0.003203716380539845, σ=0.0003874582202578705)
 Normal{Float64}(μ=0.068935978853915, σ=0.0005932645609833954)
 Normal{Float64}(μ=0.0004937165590490622, σ=0.00034980544552711034)
 Normal{Float64}(μ=-0.0034229375296490616, σ=0.0004060240358432208)
 Normal{Float64}(μ=-0.014041440888831858, σ=0.00043789872608950855)</code></pre>
</div>
</div>
</section>
</section>


</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>